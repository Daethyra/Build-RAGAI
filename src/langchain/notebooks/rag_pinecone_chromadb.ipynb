{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0d85fde0",
      "metadata": {},
      "source": [
        "# RAG with Pinecone and Chroma\n",
        "\n",
        "## Similarity Searching using top_k\n",
        "\n",
        "In this notebook, you will learn to load data, split it, retrieve and store embeddings in either Pinecone or ChromaDB, and perform similarity searching which is a way to ask your documents questions.\n",
        "\n",
        "Covered topics:\n",
        "  - LangChain\n",
        "    - Document Loaders\n",
        "    - Text Splitters\n",
        "    - Chat Models\n",
        "  - Retrieval Augmented Generation (RAG)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fac23ecb",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb759d09",
      "metadata": {},
      "source": [
        "Begin by installing all required Python libraries and `git` cloning a repository. The 'langchain-tutorials' repository is what inspired this notebook as much of the work contained herein was pulled from Greg's work. Shoutsout!!\n",
        "\n",
        "If you'd like to use your own data, comment the `git clone` command and update the `file_path` in `TextLoader` appropriately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "9d615a77",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d615a77",
        "outputId": "97147eb9-1846-4411-c649-293732203fba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'langchain-tutorials' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "%pip install -qU langchain pinecone-client python-dotenv \\\n",
        "  openai cohere tiktoken chromadb\n",
        "# Version: 0.0.164\n",
        "\n",
        "!git clone https://github.com/gkamradt/langchain-tutorials.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "2d3e92ed",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2d3e92ed",
        "outputId": "ee1273db-f3ca-4755-ec54-dad5463e35b8"
      },
      "outputs": [],
      "source": [
        "# PDF Loaders. If unstructured gives you a hard time\n",
        "from langchain.document_loaders import UnstructuredPDFLoader, OnlinePDFLoader, PyPDFLoader, TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "#from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "#load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5166d759",
      "metadata": {
        "id": "5166d759"
      },
      "source": [
        "### Stage the Data Loader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1277dcc5",
      "metadata": {},
      "source": [
        "Customize the file path of the data you'd like to load.\n",
        "\n",
        "If you're in Google Colab, prepend the below path with 'content/' and you should be good to go if you cloned the repo above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "b4a2d6bf",
      "metadata": {
        "id": "b4a2d6bf"
      },
      "outputs": [],
      "source": [
        "loader = TextLoader(file_path=\"langchain-tutorials/data/PaulGrahamEssays/vb.txt\")\n",
        "\n",
        "## Other options for loaders \n",
        "# loader = PyPDFLoader(\"../data/field-guide-to-data-science.pdf\")\n",
        "# loader = UnstructuredPDFLoader(\"../data/field-guide-to-data-science.pdf\")\n",
        "# loader = OnlinePDFLoader(\"https://wolfpaulus.com/wp-content/uploads/2017/05/field-guide-to-data-science.pdf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "bcdac23c",
      "metadata": {
        "id": "bcdac23c"
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "data = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d4810b7",
      "metadata": {},
      "source": [
        "Let's take a look at the data we loaded before processing it further."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "b4fd7c9e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4fd7c9e",
        "outputId": "04512c6b-9503-4d14-e479-d68dccf51892"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You have 1 document(s) in your data\n",
            "There are 9156 characters in your sample document\n",
            "Here is a sample: January 2016Life is short, as everyone knows. When I was a kid I used to wonder\n",
            "about this. Is life actually short, or are we really complaining\n",
            "about its finiteness?  Would we be just as likely to fe\n"
          ]
        }
      ],
      "source": [
        "# Note: If you're using PyPDFLoader then it will split by page for you already\n",
        "print (f'You have {len(data)} document(s) in your data')\n",
        "print (f'There are {len(data[0].page_content)} characters in your sample document')\n",
        "print (f'Here is a sample: {data[0].page_content[:200]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8af9b604",
      "metadata": {
        "id": "8af9b604"
      },
      "source": [
        "### Chunk your data up into smaller documents"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c95f0aa3",
      "metadata": {},
      "source": [
        "While we could pass the entire essay to a model w/ long context, we want to be picky about which information we share with our model. The better signal to noise ratio we have the more likely we are to get the right answer.\n",
        "\n",
        "The first thing we'll do is chunk up our document into smaller pieces. The goal will be to take only a few of those smaller pieces and pass them to the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "fb3c6f02",
      "metadata": {
        "id": "fb3c6f02"
      },
      "outputs": [],
      "source": [
        "# Note: If you're using PyPDFLoader then we'll be splitting for the 2nd time.\n",
        "# This is optional, test out on your own data.\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=128)\n",
        "texts = text_splitter.split_documents(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "879873a4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "879873a4",
        "outputId": "59fd6073-d29a-47fb-c738-156226ef2f73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Now you have 23 documents\n"
          ]
        }
      ],
      "source": [
        "# Let's see how many small chunks we have\n",
        "print (f'Now you have {len(texts)} documents')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "838b2843",
      "metadata": {
        "id": "838b2843"
      },
      "source": [
        "### Create embeddings of your documents to get ready for semantic search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "373e695a",
      "metadata": {
        "id": "373e695a"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import Chroma, Pinecone\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "import pinecone"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "884e7857",
      "metadata": {
        "id": "884e7857"
      },
      "source": [
        "Check to see if there is an environment variable with you API keys, if not, use what you put below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "42a1d5c3",
      "metadata": {
        "hide_input": false,
        "id": "42a1d5c3"
      },
      "outputs": [],
      "source": [
        "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY', '')\n",
        "\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b73d8504",
      "metadata": {
        "id": "b73d8504"
      },
      "source": [
        "### Option #1: Pinecone\n",
        "If you want to use pinecone, run the code below, if not then skip over to Chroma below it. You must go to [Pinecone.io](https://www.pinecone.io/) and set up an account"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "0e093ef3",
      "metadata": {
        "hide_input": false,
        "id": "0e093ef3"
      },
      "outputs": [],
      "source": [
        "PINECONE_API_KEY = os.getenv('PINECONE_API_KEY', '')\n",
        "PINECONE_API_ENV = os.getenv('PINECONE_API_ENV', 'us-central1-gcp') # You may need to switch with your env\n",
        "\n",
        "# initialize pinecone\n",
        "pinecone.init(\n",
        "    api_key=PINECONE_API_KEY,  # find at app.pinecone.io\n",
        "    environment=PINECONE_API_ENV  # next to api key in console\n",
        ")\n",
        "\n",
        "# Create the index\n",
        "pinecone.create_index('langchaintest', dimension=1536)\n",
        "\n",
        "# connect to index\n",
        "index_name = \"langchaintest\"\n",
        "\n",
        "# Iteratively embed each page's content\n",
        "docsearch = Pinecone.from_texts([t.page_content for t in texts], embeddings, index_name=index_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76d66c06",
      "metadata": {
        "id": "76d66c06"
      },
      "source": [
        "### Option #2: Chroma\n",
        "\n",
        "Chroma is a private and easy vectorstore to set up without an account"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "4e0d1c6a",
      "metadata": {
        "id": "4e0d1c6a"
      },
      "outputs": [],
      "source": [
        "# load it into Chroma\n",
        "vectorstore = Chroma.from_documents(texts, embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fdbe220",
      "metadata": {},
      "source": [
        "Lets test it out by searching for the most closely related documents to our query:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "34929595",
      "metadata": {
        "id": "34929595"
      },
      "outputs": [],
      "source": [
        "# In our code, ChromaDB uses `vectorstore`\n",
        "query = \"What is great about having kids?\"\n",
        "docs = vectorstore.similarity_search(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "4e0f5b45",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e0f5b45",
        "outputId": "187e38bc-2a34-4096-f41d-90b5bb9dd2a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "how it tricks you.  The area under the curve is small, but its shape\n",
            "jabs into your consciousness like a pin.The things that matter aren't necessarily the ones people would\n",
            "call \"important.\"  Having coffee with a friend matters.  You won't\n",
            "feel later like that was a waste of time.One great thing about having small children is that they make you\n",
            "spend time on things that matter: them. They grab your sleeve as\n",
            "you're staring at your phone and say \"will you play with me?\" And\n",
            "\n",
            "the question, and the answer is that life actually is short.Having kids showed me how to convert a continuous quantity, time,\n",
            "into discrete quantities. You only get 52 weekends with your 2 year\n",
            "old.  If Christmas-as-magic lasts from say ages 3 to 10, you only\n",
            "get to watch your child experience it 8 times.  And while it's\n",
            "impossible to say what is a lot or a little of a continuous quantity\n",
            "like time, 8 is not a lot of something.  If you had a handful of 8\n",
            "\n",
            "January 2016Life is short, as everyone knows. When I was a kid I used to wonder\n",
            "about this. Is life actually short, or are we really complaining\n",
            "about its finiteness?  Would we be just as likely to feel life was\n",
            "short if we lived 10 times as long?Since there didn't seem any way to answer this question, I stopped\n",
            "wondering about it.  Then I had kids.  That gave me a way to answer\n",
            "the question, and the answer is that life actually is short.Having kids showed me how to convert a continuous quantity, time,\n",
            "\n",
            "done that we didn't.  My oldest son will be 7 soon.  And while I\n",
            "miss the 3 year old version of him, I at least don't have any regrets\n",
            "over what might have been.  We had the best time a daddy and a 3\n",
            "year old ever had.Relentlessly prune bullshit, don't wait to do things that matter,\n",
            "and savor the time you have.  That's what you do when life is short.Notes[1]\n",
            "At first I didn't like it that the word that came to mind was\n",
            "one that had other meanings.  But then I realized the other meanings\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Here's an example of the first document that was returned\n",
        "for doc in docs:\n",
        "    print (f\"{doc.page_content}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c35dcd9",
      "metadata": {
        "id": "3c35dcd9"
      },
      "source": [
        "### Query those docs to get your answer back"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08d430c3",
      "metadata": {},
      "source": [
        "I use `ChatOpenAI` because I prefer the other classes from LangChain that go with it. You may also use an llm for the same task.\n",
        "\n",
        "<s>\n",
        "I encourage you to specify the model name because you will save money.\n",
        "\n",
        "- For example, \"gpt-3.5-turbo-1106\" is ***far far far*** cheaper than the base models.\n",
        "</s>\n",
        "\n",
        "*Base models were deprecated on January 4th, 2023.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "f051337b",
      "metadata": {
        "id": "f051337b"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains.question_answering import load_qa_chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "6b9b1c03",
      "metadata": {
        "id": "6b9b1c03"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.7, openai_api_key=OPENAI_API_KEY)\n",
        "chain = load_qa_chain(llm, chain_type=\"stuff\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "f67ea7c2",
      "metadata": {
        "id": "f67ea7c2"
      },
      "outputs": [],
      "source": [
        "# In our code, Pinecone uses `docsearch`\n",
        "query = \"What is great about having kids?\"\n",
        "docs = docsearch.similarity_search(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "3dfd2b7d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3dfd2b7d",
        "outputId": "9e1c3694-18ca-4c2d-fb17-b6ae07645920"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'One great thing about having small children is that they make you spend time on things that matter: them. They grab your sleeve as you\\'re staring at your phone and say \"will you play with me?\" This helps you focus on the important things in life and spend quality time with your children.'"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Run a chain to perform RAG\n",
        "chain.run(input_documents=docs, question=query)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
